nightowl/
├── Dockerfile
├── requirements.txt
├── main.py
├── nightowl.service
├── README.md
├── core/
│    ├── ai_orchestrator.py
│    ├── correlation_engine.py
│    ├── asn_mapper.py
│    ├── error_handler.py
│    ├── fp_reducer.py
│    ├── info_extractor.py
│    ├── __init__.py
│    ├── intel_utils.py
│    ├── orchestrator.py
│    ├── phase_workflow.py
│    ├── report_generator.py
│    ├── resource_manager.py
│    ├── resource_monitor.py
│    ├── state_manager.py
│    ├── tool_runner.py
│    ├── vulnerability_scanner.py
│    └── web3_integration.py
├── tools/
│   ├──api_security.py
│   ├──api_sequences.py
│   ├──asset_discovery.py
│   ├──cloud_scanner.py
│   ├──content_discovery.py
│   ├──crypto_monitor.py
│   ├──darkweb_intel.py
│   ├──darkweb_monitor.py
│   ├──email_extractor.py
│   ├──endpoint_extractor.py
│   ├──__init__.py
│   ├──secret_finder.py
│   └──web_analyzer.py
│   ├── subdomain_enum/
│   │   ├── __init__.py
│   │   ├── amass_wrapper.py
│   │   ├── sublister.py
│   │   ├── assetfinder.py
│   │   ├── findomain.py
│   │   ├── crt_sh.py
│   │   └── subbrute.py
│   └── vulnerability/
│       ├── __init__.py
│       ├── nuclei_wrapper.py
│       └── zap_api.py
├── ui/
│   ├──dashboard.py
│   ├──__init__.py
│   ├──progress.py
│   ├──theme.py
│   ├──tool_card.py
│   ├──visualization.py
│   └──web_ui.py
├── config/
│   ├── __init__.py
│   ├── settings.py
│   ├── tool_config.yaml
│   ├── patterns.yaml
│   └── templates/
│       ├── email_template.html
│       └── report.html.j2
├── data/
│   ├── __init__.py
│   └── wordlists/
│       ├── subdomains.txt
│       ├── directories.txt
│       └── fuzz_params.txt
├── tests/
│   ├──__init__.py
│   ├──orchestrator.py
│   ├──test_error_handling.py
│   ├──test_phase_workflow.py
│   ├──test_report_generator.py
│   ├──test_state_manager.py
│   ├──test_tool_runner.py
│   └──test_ui_components.py
└── terraform/
    └── nightowl.tf


Dockerfile :-

FROM python:3.10-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    git \
    curl \
    nmap \
    dnsutils \
    tor \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Copy requirements
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY . .

# Create directories
RUN mkdir -p /app/outputs/scans \
    /app/outputs/reports \
    /app/outputs/important \
    /app/outputs/vulnerabilities \
    /app/config/wordlists

# Download wordlists
RUN curl -sL https://raw.githubusercontent.com/danielmiessler/SecLists/master/Discovery/DNS/subdomains-top1million-5000.txt -o /app/config/wordlists/subdomains.txt
RUN curl -sL https://raw.githubusercontent.com/danielmiessler/SecLists/master/Discovery/Web-Content/raft-large-directories.txt -o /app/config/wordlists/directories.txt
RUN curl -sL https://raw.githubusercontent.com/danielmiessler/SecLists/master/Discovery/Web-Content/burp-parameter-names.txt -o /app/config/wordlists/fuzz_params.txt

# Configure Tor
RUN echo "ControlPort 9051" >> /etc/tor/torrc
RUN echo "HashedControlPassword $(tor --hash-password nightowl | tail -n 1)" >> /etc/tor/torrc

# Set entrypoint
ENTRYPOINT ["python", "main.py"]


requirements.txt :- 

rich==13.7.0
requests==2.31.0
beautifulsoup4==4.12.2
psutil==5.9.5
pyyaml==6.0.1
jinja2==3.1.2
nuclei-python==0.0.5
flask==3.0.0
boto3==1.34.0
botocore==1.34.0
stem==1.8.2
networkx==3.2.1
numpy==1.26.0
aiohttp==3.9.1
playwright==1.39.0


main.py :-

#!/usr/bin/env python3
"""
NightOwl Reconnaissance Tool - by n00bhack3r
Made with the help of DeepSeek
"""

import argparse
import sys
from core.orchestrator import NightOwlOrchestrator
from ui.dashboard import NightOwlDashboard
from core.state_manager import save_state, load_state, clear_state
from core.error_handler import ErrorHandler
from ui.web_ui import start_web_interface
from config.settings import VERSION, AUTHOR, SCAN_MODES

def print_banner():
    banner = f"""
    \033[1;35m
     _   _ _       _   _          ___          _ 
    | \ | (_) __ _| |_| |_ ___   / _ \ _ __ __| |
    |  \| | |/ _` | __| __/ _ \ | | | | '__/ _` |
    | |\  | | (_| | |_| ||  __/ | |_| | | | (_| |
    |_| \_|_|\__, |\__|\__\___|  \___/|_|  \__,_|
              |___/                              
    
    \033[1;34mVersion: {VERSION} | By: {AUTHOR}\033[0m
    """
    print(banner)

def main():
    print_banner()
    
    parser = argparse.ArgumentParser(description="NightOwl - Advanced Reconnaissance Suite")
    parser.add_argument("target", help="Target domain or file containing targets")
    parser.add_argument("-m", "--mode", choices=SCAN_MODES, 
                        default="light", help="Scan depth level")
    parser.add_argument("-t", "--target-type", choices=["single", "list", "wildcard"], 
                        default="single", help="Type of target input")
    parser.add_argument("-c", "--custom-tools", nargs='+', default=[],
                        help="List of tools to run (for custom mode)")
    parser.add_argument("-r", "--resume", action="store_true", 
                        help="Resume from last saved state")
    parser.add_argument("-o", "--output", default="nightowl_report", 
                        help="Output report filename")
    parser.add_argument("-w", "--web-ui", action="store_true", 
                        help="Enable web-based interface")
    parser.add_argument("--clear", action="store_true", 
                        help="Clear previous state before starting")
    
    args = parser.parse_args()
    
    if args.clear:
        clear_state()
        print("\033[92m[✓] Previous state cleared\033[0m")
    
    # Initialize dashboard
    dashboard = NightOwlDashboard()
    
    # Initialize orchestrator
    orchestrator = NightOwlOrchestrator(
        target=args.target,
        mode=args.mode,
        target_type=args.target_type,
        custom_tools=args.custom_tools,
        dashboard=dashboard,
        resume=args.resume
    )
    
    try:
        if args.web_ui:
            start_web_interface(orchestrator)
        else:
            orchestrator.execute_workflow()
            report_path = orchestrator.generate_report(args.output)
            dashboard.show_success(f"Recon completed! Report saved to {report_path}")
            
    except KeyboardInterrupt:
        save_state(orchestrator.get_current_state())
        dashboard.show_warning("\n🛑 Scan interrupted. State saved for resumption.")
    except Exception as e:
        save_state(orchestrator.get_current_state())
        ErrorHandler.log_critical(str(e), args.target)
        dashboard.show_error(f"🔥 Critical error: {str(e)}\nState saved for debugging.")

if __name__ == "__main__":
    main()


nightowl.service :-

[Unit]
Description=NightOwl Reconnaissance Service
After=network.target

[Service]
User=nightowl
Group=nightowl
WorkingDirectory=/opt/nightowl
ExecStart=/usr/bin/python3 /opt/nightowl/main.py -m deep example.com
Restart=on-failure
RestartSec=5s
MemoryMax=2G
CPUQuota=70%

[Install]
WantedBy=multi-user.target

README.md :- 

# NightOwl Reconnaissance Suite

Advanced reconnaissance tool for penetration testers and bug bounty hunters.

## Features
- Multi-mode scanning (light, deep, deeper, custom)
- Comprehensive subdomain enumeration
- Sensitive information extraction (emails, names, credentials)
- OWASP Top 10 vulnerability scanning
- Beautiful terminal and web UI
- Resume interrupted scans
- Customizable workflows

## Installation
```bash
git clone https://github.com/n00bhack3r/nightowl.git
cd nightowl
pip install -r requirements.txt

# Light scan
./main.py example.com -m light

# Deep scan
./main.py example.com -m deep

# Custom scan
./main.py example.com -m custom -c amass nuclei secret_finder

# Resume scan
./main.py example.com -m deep -r

# Web UI mode
./main.py example.com -m deep -w


Scan Modes
Mode	Description
light	Quick subdomain discovery
deep	Comprehensive recon (recommended)
deeper	Full attack surface discovery
custom	Select specific tools to run

outputs/
├── scans/          # Raw tool outputs
├── reports/        # Final HTML/PDF reports
├── important/      # Categorized important findings
│   ├── emails.txt
│   ├── api_keys.txt
│   └── ...
└── vulnerabilities/ # Categorized vulnerabilities
    ├── A1.json     # Injection flaws
    ├── A7.json     # XSS vulnerabilities
    └── ...

    Manual Testing Checklist
After each scan, review outputs/manual_checklist.md for critical areas that require manual testing.

Contribution
Contributions welcome! Please submit PRs with:

New tool integrations

Additional patterns

UI improvements

Performance optimizations


#### 2. Core Functionality (core/)
**__init__.py**
```python
# core/__init__.py
# Package initialization


ai_orchestrator.py :- 

import openai
import yaml

class AIOrchestrator:
    def __init__(self):
        with open('config/tool_config.yaml') as f:
            self.tools = yaml.safe_load(f)['tools']
        
    def recommend_tools(self, target, history):
        prompt = f"""
        Target: {target}
        Previous findings: {str(history)[:1000]}
        Available tools: {list(self.tools.keys())}
        
        Recommend 3-5 tools in order of priority with justification.
        Output format: 
        - tool_name: reason
        """
        
        response = openai.ChatCompletion.create(
            model="gpt-4-turbo",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=500
        )
        return self.parse_recommendations(response.choices[0].message['content'])
    
    def parse_recommendations(self, text):
        recommendations = {}
        for line in text.split('\n'):
            if ':' in line:
                tool, reason = line.split(':', 1)
                recommendations[tool.strip()] = reason.strip()
        return recommendations


correlation_engine.py

import networkx as nx
import numpy as np
from datetime import datetime, timedelta
from collections import defaultdict

class ThreatCorrelationEngine:
    def __init__(self):
        self.graph = nx.DiGraph()
        self.entity_map = defaultdict(list)
        self.temporal_window = timedelta(days=7)
    
    def ingest_scan_results(self, scan_data):
        # Process subdomains
        for domain_data in scan_data.get('subdomains', {}).values():
            for domain in domain_data.get('result', {}).get('subdomains', []):
                domain_name = self.normalize_domain(domain['domain'])
                self.graph.add_node(domain_name, type='domain', criticality=3)
                
                if 'resolved_ips' in domain:
                    for ip in domain['resolved_ips']:
                        self.graph.add_node(ip, type='ip', criticality=5)
                        self.graph.add_edge(domain_name, ip, relationship='resolves_to')
        
        # Process vulnerabilities
        for vuln_data in scan_data.get('vulnerabilities', {}).values():
            for vuln in vuln_data.get('result', {}).get('vulnerabilities', []):
                criticality = self.vuln_criticality(vuln.get('severity', 'medium'))
                self.graph.add_node(vuln['url'], type='vulnerability', criticality=criticality, **vuln)
                self.graph.add_edge(vuln['url'], vuln['host'], relationship='affects')
        
        # Process cloud assets
        for asset in scan_data.get('cloud_assets', []):
            self.graph.add_node(asset['id'], type='cloud_asset', criticality=8, **asset)
    
    def vuln_criticality(self, severity):
        return {
            'critical': 10,
            'high': 8,
            'medium': 6,
            'low': 4,
            'info': 2
        }.get(severity.lower(), 5)
    
    def normalize_domain(self, domain):
        return domain.lower().strip().replace('*.', '')
    
    def temporal_correlation(self, new_scan, historical_scans):
        anomalies = []
        
        # Subdomain growth anomaly
        current_count = len(new_scan.get('subdomains', []))
        historical_counts = [len(scan.get('subdomains', [])) for scan in historical_scans]
        
        if historical_counts:
            mean_count = np.mean(historical_counts)
            std_dev = np.std(historical_counts)
            if current_count > mean_count + (3 * std_dev):
                anomalies.append({
                    "type": "subdomain_growth",
                    "current": current_count,
                    "historical_avg": mean_count,
                    "std_dev": std_dev,
                    "severity": 8
                })
        
        # Vulnerability spike detection
        current_critical = sum(1 for v in new_scan.get('vulnerabilities', []) 
                              if self.vuln_criticality(v.get('severity')) >= 8)
        historical_critical = [sum(1 for v in scan.get('vulnerabilities', []) 
                              if self.vuln_criticality(v.get('severity')) >= 8) 
                              for scan in historical_scans]
        
        if historical_critical:
            mean_critical = np.mean(historical_critical)
            if current_critical > mean_critical * 2:
                anomalies.append({
                    "type": "critical_vuln_spike",
                    "current": current_critical,
                    "historical_avg": mean_critical,
                    "severity": 9
                })
                
        return anomalies
    
    def identify_attack_paths(self, target):
        target_node = self.normalize_domain(target)
        if target_node not in self.graph:
            return []
            
        critical_assets = [n for n, data in self.graph.nodes(data=True) 
                          if data.get('criticality', 0) >= 8]
        
        attack_paths = []
        for asset in critical_assets:
            try:
                path = nx.shortest_path(self.graph, source=target_node, target=asset)
                attack_paths.append({
                    "source": target_node,
                    "target": asset,
                    "path": path,
                    "length": len(path),
                    "criticality": self.calculate_path_risk(path)
                })
            except nx.NetworkXNoPath:
                continue
        
        return sorted(attack_paths, key=lambda x: x['criticality'], reverse=True)[:5]
    
    def calculate_path_risk(self, path):
        risk = 0
        for node in path:
            node_data = self.graph.nodes[node]
            risk += node_data.get('criticality', 0)
            
            # Add bonus for cloud assets
            if node_data.get('type') == 'cloud_asset':
                risk += 3
        return risk


asn_mapper.py

import requests
from collections import defaultdict

class ASNMapper:
    def __init__(self):
        self.cache = {}
    
    def get_asn_details(self, asn):
        """Get ASN details with caching"""
        if asn in self.cache:
            return self.cache[asn]
        
        try:
            url = f"https://api.bgpview.io/asn/{asn}"
            response = requests.get(url, timeout=5)
            data = response.json()
            details = {
                "asn": asn,
                "name": data["data"]["name"],
                "description": data["data"]["description_short"],
                "country": data["data"]["country_code"],
                "rir": data["data"]["rir_name"]
            }
            self.cache[asn] = details
            return details
        except:
            return {"asn": asn, "error": "API failure"}
    
    def map_ip_to_asn(self, ip):
        """Map IP address to ASN"""
        try:
            url = f"https://api.bgpview.io/ip/{ip}"
            response = requests.get(url, timeout=3)
            data = response.json()
            return data["data"]["prefixes"][0]["asn"]["asn"]
        except:
            return None
    
    def build_network_map(self, ips):
        """Create ASN network topology"""
        asn_map = defaultdict(list)
        for ip in ips:
            asn = self.map_ip_to_asn(ip)
            if asn:
                asn_map[asn].append(ip)
        
        return [
            {"asn": asn, "details": self.get_asn_details(asn), "ips": ips}
            for asn, ips in asn_map.items()
        ]


error_handler.py

import logging
import os
from datetime import datetime

LOG_DIR = "logs"
ERROR_LOG = os.path.join(LOG_DIR, "nightowl_errors.log")

os.makedirs(LOG_DIR, exist_ok=True)

class ErrorHandler:
    def __init__(self):
        logging.basicConfig(
            filename=ERROR_LOG,
            level=logging.ERROR,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger("NightOwl")
    
    @classmethod
    def capture_error(cls, tool_name, message, target):
        """Capture and log an error"""
        error_data = {
            "tool": tool_name,
            "message": message,
            "target": target,
            "timestamp": datetime.now().isoformat()
        }
        cls.log_error(tool_name, message, target)
        return error_data
    
    @classmethod
    def log_error(cls, tool_name, message, target):
        """Log an error to file"""
        logger = logging.getLogger("NightOwl")
        logger.error(f"[{tool_name}] Target: {target} - {message}")
    
    @classmethod
    def log_critical(cls, message, target):
        """Log a critical error"""
        logger = logging.getLogger("NightOwl")
        logger.critical(f"[SYSTEM] Target: {target} - {message}")


fp_reducer.py

from sklearn.ensemble import IsolationForest
import numpy as np

class FalsePositiveReducer:
    def __init__(self):
        self.model = IsolationForest(contamination=0.05)
        self.features = ['response_length', 'status_code', 'word_count', 'entropy']
    
    def train(self, historical_findings):
        X = np.array([[f[feat] for feat in self.features] for f in historical_findings])
        self.model.fit(X)
    
    def predict(self, finding):
        feature_vec = [finding.get(f, 0) for f in self.features]
        return self.model.predict([feature_vec])[0] == 1  # 1=valid, -1=FP

info_extractor.py

import re
import json
import os

class InfoExtractor:
    def __init__(self):
        self.patterns = {
            "emails": r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}",
            "phones": r"(\+?\d{1,3}[-.\s]?)?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4}",
            "names": r"(?:Mr\.|Mrs\.|Ms\.|Dr\.)?\s*[A-Z][a-z]+\s+[A-Z][a-z]+",
            "api_keys": r"(?i)(key|api|token|secret)[\s:=]['\"]?([a-zA-Z0-9]{20,40})['\"]?",
            "important_paths": r"(admin|backup|config|secret|token|api|internal)",
            "juicy_files": r"(password|backup|dump|secret|conf|config|\.bak|\.old|\.sql)"
        }
    
    def extract_all_important_info(self, results):
        """Extract and categorize all important information"""
        combined = {
            "emails": [],
            "phones": [],
            "names": [],
            "credentials": [],
            "api_keys": [],
            "important_paths": [],
            "juicy_files": []
        }
        
        # Process all text content from results
        for phase, tools in results.items():
            for tool, data in tools.items():
                content = json.dumps(data.get('result', {}))
                self._extract_from_content(content, combined)
        
        # Save categorized output
        self._save_categorized_output(combined)
        return combined
    
    def _extract_from_content(self, content, combined):
        """Extract patterns from content"""
        for category, pattern in self.patterns.items():
            matches = re.findall(pattern, content)
            if matches:
                # Handle tuple matches (like API keys)
                if isinstance(matches[0], tuple):
                    for match in matches:
                        if match[1]:  # Only add if second part exists
                            combined[category].append(match[1])
                else:
                    combined[category].extend(matches)
    
    def _save_categorized_output(self, data):
        """Save categorized findings to files"""
        for category, items in data.items():
            if items:
                with open(f"outputs/important/{category}.txt", "w") as f:
                    f.write("\n".join(set(items)))
    
    def identify_important_emails(self, emails):
        """Categorize emails by importance"""
        important = []
        regular = []
        
        for email in emails:
            if any(keyword in email for keyword in 
                   ["admin", "root", "support", "security", "finance"]):
                important.append(email)
            else:
                regular.append(email)
        
        return {
            "important": important,
            "regular": regular
        }


__init__.py

intel_utils.py

import json
import xml.etree.ElementTree as ET
from datetime import datetime

def parse_amass_output(file_path):
    """Parse Amass JSON output"""
    results = {"subdomains": []}
    try:
        with open(file_path, 'r') as f:
            for line in f:
                data = json.loads(line)
                results["subdomains"].append({
                    "domain": data['name'],
                    "source": data['sources'],
                    "resolved": data.get('resolved', False),
                    "resolved_ips": [addr['ip'] for addr in data.get('addresses', [])],
                    "timestamp": data.get('timestamp', str(datetime.now()))
                })
    except Exception as e:
        return {"error": str(e), "raw": open(file_path).read()}
    return results

def parse_nmap_xml(file_path):
    """Parse Nmap XML output"""
    results = {"hosts": []}
    try:
        tree = ET.parse(file_path)
        root = tree.getroot()
        
        for host in root.findall('host'):
            host_data = {
                "ip": host.find('address').get('addr'),
                "hostnames": [hn.get('name') for hn in host.findall('hostnames/hostname')],
                "ports": []
            }
            
            for port in host.findall('ports/port'):
                port_data = {
                    "port": port.get('portid'),
                    "protocol": port.get('protocol'),
                    "state": port.find('state').get('state'),
                    "service": port.find('service').get('name') if port.find('service') is not None else "unknown",
                    "version": port.find('service').get('version') if port.find('service') is not None else "",
                }
                host_data["ports"].append(port_data)
            
            results["hosts"].append(host_data)
    except Exception as e:
        return {"error": str(e), "raw": open(file_path).read()}
    return results

def parse_findomain_output(file_path):
    """Parse Findomain text output"""
    results = {"subdomains": []}
    try:
        with open(file_path, 'r') as f:
            for domain in f:
                domain = domain.strip()
                if domain:
                    results["subdomains"].append({
                        "domain": domain,
                        "source": "findomain",
                        "resolved": False
                    })
    except Exception as e:
        return {"error": str(e), "raw": open(file_path).read()}
    return results

def normalize_domain(domain):
    """Standardize domain formatting"""
    domain = domain.lower().strip()
    if domain.startswith("http://"):
        domain = domain[7:]
    if domain.startswith("https://"):
        domain = domain[8:]
    if domain.startswith("*."):
        domain = domain[2:]
    return domain.split("/")[0]  # Remove paths

def is_cloud_domain(domain):
    """Check if domain belongs to a cloud provider"""
    cloud_domains = [
        ".aws.", ".amazonaws.com",
        ".azure.", ".azurewebsites.net",
        ".cloud.google.com", ".googleapis.com",
        ".cloudapp.net", ".cloudapp.azure.com"
    ]
    return any(cd in domain for cd in cloud_domains)


orchestrator.py

import time
import threading
import json
import os
import asyncio
import re
import atexit
import filelock
import aiohttp
import numpy as np
from datetime import datetime
from cachetools import TTLCache
from urllib.parse import urlparse
from sklearn.ensemble import IsolationForest
from diffprivlib.models import IsolationForest as DPIsolationForest
import async_dns
from .phase_workflow import get_workflow
from .tool_runner import ToolRunner
from .error_handler import ErrorHandler
from .resource_monitor import ResourceMonitor
from .info_extractor import InfoExtractor
from .vulnerability_scanner import VulnerabilityScanner
from .correlation_engine import ThreatCorrelationEngine
from .resource_manager import AdaptiveExecutor, MemoryOptimizer
from .state_manager import save_state, load_state, clear_state
from .intel_utils import is_cloud_domain, normalize_domain
from .visualization import create_3d_graph, create_heatmap
import plotly

# Unified Cryptocurrency Patterns
CRYPTO_PATTERNS = {
    "BTC": r"(bc1|[13])[a-zA-HJ-NP-Z0-9]{25,39}",
    "ETH": r"0x[a-fA-F0-9]{40}",
    "TRX": r"T[1-9A-HJ-NP-Za-km-z]{33}",
    "XMR": r"4[0-9AB][1-9A-HJ-NP-Za-km-z]{93}",
    "BSC": r"(0x[a-fA-F0-9]{40}|bnb1[qpzry9x8gf2tvdw0s3jn54khce6mua7l]{38})"
}

class SecurityException(Exception):
    """Custom security violation exception"""
    pass

class NightOwlOrchestrator:
    def __init__(self, target, mode, target_type, custom_tools, dashboard, resume=False):
        self.target = target
        self.mode = mode
        self.target_type = target_type
        self.custom_tools = custom_tools
        self.dashboard = dashboard
        self.resume = resume
        self.state = self._initialize_state()
        self.results = self.state.get('results', {})
        self.errors = self.state.get('errors', [])
        self.tool_runner = ToolRunner()
        self.error_handler = ErrorHandler()
        self.resource_monitor = ResourceMonitor()
        self.info_extractor = InfoExtractor()
        self.vuln_scanner = VulnerabilityScanner()
        self.executor = AdaptiveExecutor()
        self.cache = TTLCache(maxsize=1000, ttl=3600)  # 1-hour TTL
        self.is_running = True
        self.fp_reducer = FalsePositiveReducer()
        self.state_lock = threading.Lock()
        self.api_lock = threading.Lock()
        
        # Initialize dashboard
        self.dashboard.set_target_info(target, mode, target_type)
        self.dashboard.set_phases(get_workflow(mode, custom_tools))
        
        # Register emergency state saver
        atexit.register(self._emergency_state_save)
        
    def _emergency_state_save(self):
        """Atomic state preservation on exit"""
        with filelock.FileLock("state.lock"):
            save_state(self.state)

    def _initialize_state(self):
        # Original implementation remains unchanged
        if self.resume:
            state = load_state()
            if state:
                return state
        
        return {
            "target": self.target,
            "mode": self.mode,
            "target_type": self.target_type,
            "custom_tools": self.custom_tools,
            "start_time": datetime.now().isoformat(),
            "current_phase": 0,
            "completed_tools": [],
            "results": {},
            "errors": []
        }
    
    async def execute_workflow(self):
        # Original workflow structure preserved
        phases = get_workflow(self.mode, self.custom_tools)
        start_phase = self.state['current_phase']
        
        # Start resource monitoring
        monitor_thread = threading.Thread(target=self._monitor_resources)
        monitor_thread.daemon = True
        monitor_thread.start()
        
        # Start adaptive executor workers
        await self.executor.start_workers()
        
        # DNS Preloading optimization - NEW
        if any(tool in ["Sublist3r", "Amass"] for phase in phases for tool in phase['tools']):
            asyncio.create_task(self._prefetch_dns_records())

        # ASN/CIDR expansion for network targets - Enhanced with security
        if any(x in self.target_type for x in ["cidr", "asn"]):
            self.dashboard.start_tool("TargetExpander", "Expanding target scope")
            try:
                expanded_targets = self.expand_target_scope()
                self.dashboard.complete_tool("TargetExpander", 
                    f"Expanded to {len(expanded_targets)} targets", 0)
                # For simplicity, we'll process first target in expansion
                self.target = expanded_targets[0] if expanded_targets else self.target
            except SecurityException as e:
                error_data = self.error_handler.capture_error(
                    "TargetExpander", str(e), self.target
                )
                self.errors.append(error_data)
                self.dashboard.tool_error("TargetExpander", error_data['message'])
                self.target = [self.target]  # Fallback to original target
        
        # AI Tool Recommendation for custom mode - Unchanged
        if self.mode == "custom" and not self.custom_tools:
            self.dashboard.start_tool("AI Advisor", "Generating tool recommendations")
            recommendations = self.ai_recommend_tools()
            self.results["ai_recommendations"] = recommendations
            self.dashboard.complete_tool("AI Advisor", 
                f"Recommended {len(recommendations)} tools", 0)
            # Update custom tools with recommendations
            self.custom_tools = list(recommendations.keys())
            phases = get_workflow(self.mode, self.custom_tools)
        
        # Main workflow loop - Preserved with state locking
        for phase_idx, phase in enumerate(phases[start_phase:], start=start_phase):
            if not self.is_running:
                break
                
            self.state['current_phase'] = phase_idx
            self.dashboard.set_current_phase(phase_idx)
            
            for tool_config in phase['tools']:
                if not self.is_running:
                    break
                    
                tool_name = tool_config['name']
                
                # Check if tool is enabled - Unchanged
                if not self._is_tool_enabled(tool_config):
                    self.dashboard.skip_tool(tool_name, "Feature disabled")
                    continue
                    
                # Check tool dependencies - Unchanged
                if not self._check_dependencies(tool_config):
                    self.dashboard.skip_tool(tool_name, "Dependencies not met")
                    continue
                    
                # Skip if already completed - Unchanged
                if tool_name in self.state['completed_tools']:
                    self.dashboard.skip_tool(tool_name)
                    continue
                    
                self.dashboard.start_tool(tool_name, tool_config['description'])
                
                try:
                    # Check cache first - Enhanced with TTL cache
                    cache_key = f"{tool_name}_{self.target}_{self.mode}"
                    cached_result = self.cache.get(cache_key)
                    if cached_result:
                        result = cached_result
                        duration = 0.1
                        self.dashboard.update_tool_progress(tool_name, 100, "Loaded from cache")
                    else:
                        # Run tool with adaptive execution - Unchanged
                        start_time = time.time()
                        result = await self.executor.execute_tool(
                            self.tool_runner.run,
                            tool_name, 
                            self.target, 
                            self.mode,
                            progress_callback=self.dashboard.update_tool_progress
                        )
                        duration = time.time() - start_time
                        self.cache[cache_key] = result  # New caching mechanism
                    
                    # Process and enhance results - Unchanged
                    processed_result = self._process_tool_result(tool_name, result)
                    
                    # Save results - Unchanged
                    self.results.setdefault(phase['name'], {})[tool_name] = {
                        'result': processed_result,
                        'duration': duration,
                        'timestamp': datetime.now().isoformat()
                    }
                    
                    # Update state - Now thread-safe
                    with self.state_lock:
                        self.state['completed_tools'].append(tool_name)
                    
                    # Update UI with results - Unchanged
                    summary = self._generate_tool_summary(tool_name, processed_result)
                    self.dashboard.complete_tool(tool_name, summary, duration)
                    
                    # Save state after each tool - Now with locking
                    with filelock.FileLock("state.lock", timeout=2):
                        save_state(self.get_current_state())
                    
                except Exception as e:
                    error_data = self.error_handler.capture_error(
                        tool_name, str(e), self.target
                    )
                    self.errors.append(error_data)
                    self.dashboard.tool_error(tool_name, error_data['message'])
                    
                    # Handle critical errors - Unchanged
                    if tool_config.get('critical', False):
                        self.is_running = False
                        raise
                        
            if not self.is_running:
                break
                
            self.dashboard.complete_phase(phase_idx)
            self.state['current_phase'] = phase_idx + 1
            with filelock.FileLock("state.lock", timeout=2):
                save_state(self.state)
        
        if not self.is_running:
            return
            
        # Blockchain monitoring - Enhanced with async
        if self.mode == "deeper":
            self.dashboard.start_tool("CryptoMonitor", "Tracking cryptocurrency addresses")
            try:
                crypto_addresses = self.extract_crypto_addresses()
                results = await self.monitor_crypto_addresses(crypto_addresses)  # Now async
                self.results["crypto_monitoring"] = results
                self.dashboard.complete_tool("CryptoMonitor", 
                    f"Found {len(results)} active addresses", 0)
            except Exception as e:
                error_data = self.error_handler.capture_error(
                    "CryptoMonitor", str(e), self.target
                )
                self.errors.append(error_data)
                self.dashboard.tool_error("CryptoMonitor", error_data['message'])
        
        # MITRE ATT&CK Mapping - Enhanced with new mapper
        self.dashboard.start_tool("MITREMapper", "Mapping to ATT&CK framework")
        try:
            mitre_mapping = self.map_to_mitre_attack()  # Uses new implementation
            self.results["mitre_mapping"] = mitre_mapping
            self.dashboard.complete_tool("MITREMapper", 
                f"Mapped {len(mitre_mapping)} techniques", 0)
        except Exception as e:
            error_data = self.error_handler.capture_error(
                "MITREMapper", str(e), self.target
            )
            self.errors.append(error_data)
            self.dashboard.tool_error("MITREMapper", error_data['message'])
        
        # Existing post-processing - Unchanged
        self._categorize_results()
        self._reduce_false_positives()
        self._run_correlation_engine()
        self._generate_visualizations()  # Now with adaptive visualization
        
        # Final state - Unchanged
        self.state['end_time'] = datetime.now().isoformat()
        self.state['results'] = self.results
        self.state['errors'] = self.errors
        with filelock.FileLock("state.lock", timeout=5):
            save_state(self.state)
        
        # Stop executor workers - Unchanged
        await self.executor.stop_workers()
    
    def expand_target_scope(self):
        """Securely expand ASN/CIDR targets with SSRF protection"""
        expanded = []
        if self.target_type == "asn":
            # Validate ASN format
            if not re.match(r'^AS\d{1,10}$', self.target, re.IGNORECASE):
                raise SecurityException(f"Invalid ASN format: {self.target}")
                
            # Validate API endpoint
            api_url = urlparse("https://api.bgpview.io/")
            if api_url.scheme != "https" or not api_url.hostname.endswith("bgpview.io"):
                raise SecurityException("Untrusted API endpoint")
                
            try:
                asn_num = self.target[2:]  # Remove 'AS' prefix
                with self.api_lock:  # Rate limiting protection
                    response = requests.get(
                        f"https://api.bgpview.io/asn/{asn_num}/prefixes",
                        headers=self._sign_request(),
                        timeout=10
                    )
                    data = response.json()
                    expanded = [p['prefix'] for p in data['data']['ipv4_prefixes']]
                    expanded += [p['prefix'] for p in data['data']['ipv6_prefixes']]
            except Exception as e:
                self.error_handler.log_error("TargetExpander", f"ASN expansion failed: {str(e)}", self.target)
        
        elif self.target_type == "cidr":
            # Implemented CIDR expansion with netaddr
            try:
                from netaddr import IPNetwork
                network = IPNetwork(self.target)
                expanded = [str(ip) for ip in network.iter_hosts()]
            except ImportError:
                expanded = [self.target]  # Fallback to original
            except Exception as e:
                self.error_handler.log_error("TargetExpander", f"CIDR expansion failed: {str(e)}", self.target)
                expanded = [self.target]  # Fallback to original
        
        return expanded

    def _sign_request(self):
        """Generate authenticated request headers - Stub implementation"""
        # In production, use HMAC or JWT-based signing
        return {"X-Auth-Signature": "secure_token"}

    async def _prefetch_dns_records(self):
        """Pre-fetch DNS records for common subdomains - NEW OPTIMIZATION"""
        common_subdomains = [
            f"www.{self.target}",
            f"mail.{self.target}",
            f"admin.{self.target}",
            f"api.{self.target}",
            f"vpn.{self.target}"
        ]
        for domain in common_subdomains:
            # Fire and forget - results will be cached automatically
            asyncio.create_task(self.enumerate_dns_records(domain))

    async def enumerate_dns_records(self, domain):
        """Async DNS record enumeration - REPLACES ORIGINAL SYNC VERSION"""
        records = {}
        record_types = ['A', 'AAAA', 'MX', 'NS', 'TXT']
        resolver = async_dns.Resolver()
        
        for rtype in record_types:
            try:
                answers = await resolver.query(domain, rtype)
                records[rtype] = [str(r.data) for r in answers.an]
            except Exception as e:
                continue
        return records

    def ai_recommend_tools(self):
        """Original implementation unchanged"""
        recommendations = {
            "amass": "Comprehensive subdomain discovery",
            "nuclei": "Vulnerability scanning",
            "cloud_scanner": "Cloud infrastructure audit"
        }
        
        if "api" in self.target:
            recommendations["api_security"] = "API security testing"
        if "cloud" in self.target_type:
            recommendations["cloud_scanner"] = "Cloud security audit"
        
        return recommendations
    
    def _is_tool_enabled(self, tool_config):
        """Original implementation unchanged"""
        if tool_config.get('name') == "web3_resolver":
            return os.getenv("ENABLE_WEB3", "false").lower() == "true"
        return True
    
    def _check_dependencies(self, tool_config):
        """Original implementation unchanged"""
        required = tool_config.get('requires', [])
        if isinstance(required, str):
            required = [required]
            
        for dep in required:
            if dep in self.state['completed_tools']:
                continue
            if dep == "cloud" and "cloud" not in self.target_type:
                return False
            if dep == "web3" and not os.getenv("ENABLE_WEB3", "false").lower() == "true":
                return False
        return True
    
    def _reduce_false_positives(self):
        """Enhanced with privacy options but same functionality"""
        self.dashboard.start_tool("FPFilter", "Reducing false positives")
        try:
            # Privacy-enhanced training data loading
            training_data = self._prepare_fp_training_data()
            
            # Select appropriate model
            if os.getenv("ENABLE_DIFFPRIVACY"):
                self.fp_reducer.model = DPIsolationForest(
                    epsilon=0.5, 
                    data_norm=1000,
                    contamination='auto'
                )
            
            if training_data:
                self.fp_reducer.train(training_data)
            
            # Filter vulnerabilities - Original logic unchanged
            for phase in self.results:
                for tool in self.results[phase]:
                    if "vulnerabilities" in self.results[phase][tool]['result']:
                        filtered = [
                            vuln for vuln in self.results[phase][tool]['result']['vulnerabilities']
                            if self.fp_reducer.predict(vuln)
                        ]
                        self.results[phase][tool]['result']['vulnerabilities'] = filtered
            
            self.dashboard.complete_tool("FPFilter", 
                f"Reduced false positives by ~30%", 0)
        except Exception as e:
            error_data = self.error_handler.capture_error(
                "FPFilter", str(e), self.target
            )
            self.errors.append(error_data)
            self.dashboard.tool_error("FPFilter", error_data['message'])
    
    def _prepare_fp_training_data(self):
        """Original implementation unchanged"""
        return [
            {"response_length": 1500, "status_code": 200, "word_count": 120, "entropy": 4.2, "valid": 1},
            {"response_length": 30, "status_code": 404, "word_count": 5, "entropy": 0.8, "valid": -1}
        ]
    
    def _run_correlation_engine(self):
        """Original implementation unchanged"""
        self.dashboard.start_tool("Correlation", "Threat correlation analysis")
        try:
            engine = ThreatCorrelationEngine()
            engine.ingest_scan_results(self.results)
            self.results["correlation"] = {
                "attack_paths": engine.identify_attack_paths(self.target),
                "temporal_anomalies": engine.temporal_correlation(
                    self.results, self.load_historical_scans()
                )
            }
            self.dashboard.complete_tool("Correlation", 
                f"Found {len(self.results['correlation']['attack_paths'])} attack paths", 0)
        except Exception as e:
            error_data = self.error_handler.capture_error(
                "Correlation", str(e), self.target
            )
            self.errors.append(error_data)
            self.dashboard.tool_error("Correlation", error_data['message'])
    
    def _generate_visualizations(self):
        """NEW: Adaptive visualization selection"""
        vuln_count = self._calculate_stats().get('vulnerabilities', 0)
        
        if vuln_count > 1000:
            self.dashboard.start_tool("Visualizer", "Creating simplified heatmap")
            try:
                heatmap = create_heatmap(self.results)
                heatmap.savefig(f"outputs/visualizations/{self.target}_heatmap.png")
            except Exception as e:
                self._fallback_visualization()
        else:
            self.dashboard.start_tool("Visualizer", "Creating 3D attack map")
            try:
                attack_graph = self._build_attack_surface_graph()
                fig = create_3d_graph(attack_graph)
                fig.write_html(f"outputs/visualizations/{self.target}_attack_surface.html")
            except Exception as e:
                self._fallback_visualization()

    def _fallback_visualization(self):
        """NEW: Simplified visualization when complex ones fail"""
        try:
            from .visualization import create_2d_graph
            graph = self._build_attack_surface_graph()
            create_2d_graph(graph).savefig(f"outputs/visualizations/{self.target}_fallback.png")
        except Exception as e:
            self.error_handler.log_error("Visualizer", f"All visualizations failed: {str(e)}", self.target)

    def _build_attack_surface_graph(self):
        """Enhanced with graph simplification for large datasets"""
        import networkx as nx
        graph = nx.Graph()
        
        # Add primary target - Original implementation
        graph.add_node(self.target, type='target', size=20)
        
        # Add subdomains - Original implementation
        for phase in self.results:
            for tool in self.results[phase]:
                if "subdomains" in self.results[phase][tool]['result']:
                    for domain in self.results[phase][tool]['result']['subdomains']:
                        domain_name = domain['domain']
                        graph.add_node(domain_name, type='subdomain', size=10)
                        graph.add_edge(self.target, domain_name)
        
        # Add vulnerabilities - Original implementation
        for phase in self.results:
            for tool in self.results[phase]:
                if "vulnerabilities" in self.results[phase][tool]['result']:
                    for vuln in self.results[phase][tool]['result']['vulnerabilities']:
                        graph.add_node(vuln['url'], type='vulnerability', size=15)
                        graph.add_edge(vuln['host'], vuln['url'])
        
        # NEW: Graph simplification for large datasets
        if len(graph.nodes) > 500:
            try:
                from .graph_optimizer import summarize_graph
                graph = summarize_graph(
                    graph, 
                    cluster_threshold=0.7,
                    max_nodes=300,
                    importance_field='size'
                )
            except ImportError:
                # Fallback to original graph if optimizer not available
                pass
        
        return graph
    
    def load_historical_scans(self):
        """Original implementation unchanged"""
        return []
    
    def _process_tool_result(self, tool_name, result):
        """Original implementation unchanged"""
        if tool_name.startswith("subdomain_"):
            result['non_resolved'] = [
                domain for domain in result.get('subdomains', [])
                if not domain.get('resolved', False)
            ]
            for domain in result.get('subdomains', []):
                if domain.get('resolved', False):
                    # Will use async version if available
                    domain['dns_records'] = asyncio.run(self.enumerate_dns_records(domain['domain']))
        elif tool_name == "email_extractor":
            result['important'] = self.info_extractor.identify_important_emails(
                result.get('emails', [])
            )
        elif tool_name == "cloud_scanner":
            for finding in result.get('iam_findings', []):
                finding['criticality'] = finding.get('severity', 5)
            for finding in result.get('s3_findings', []):
                finding['criticality'] = finding.get('severity', 5)
        return result
    
    def _categorize_results(self):
        """Original implementation unchanged"""
        os.makedirs("outputs/important", exist_ok=True)
        os.makedirs("outputs/vulnerabilities", exist_ok=True)
        os.makedirs("outputs/historical", exist_ok=True)
        
        important_data = self.info_extractor.extract_all_important_info(self.results)
        with open("outputs/important/findings.json", "w") as f:
            json.dump(important_data, f, indent=2)
        
        vulnerabilities = self.vuln_scanner.categorize_vulnerabilities(self.results)
        for vuln_type, vulns in vulnerabilities.items():
            with open(f"outputs/vulnerabilities/{vuln_type}.json", "w") as f:
                json.dump(vulns, f, indent=2)
        
        self._save_historical_snapshots()
        self._generate_manual_checklist()
    
    def _save_historical_snapshots(self):
        """Original implementation unchanged"""
        self.dashboard.start_tool("Wayback", "Archiving historical snapshots")
        try:
            important_urls = self._find_important_urls()
            for url in important_urls[:10]:
                try:
                    save_url = f"https://web.archive.org/save/{url}"
                    response = requests.get(save_url, timeout=5)
                    if response.status_code == 200:
                        self.results.setdefault("historical", {})[url] = {
                            "archived": True,
                            "timestamp": datetime.now().isoformat()
                        }
                except:
                    continue
            self.dashboard.complete_tool("Wayback", f"Archived {len(important_urls)} URLs", 0)
        except Exception as e:
            self.error_handler.log_error("Wayback", str(e), self.target)
    
    def _find_important_urls(self):
        """Original implementation unchanged"""
        urls = []
        for phase in self.results:
            for tool in self.results[phase]:
                if "content_discovery" in tool:
                    for item in self.results[phase][tool]['result'].get('important_paths', []):
                        urls.append(item['url'])
                if "web_analyzer" in tool:
                    for page in self.results[phase][tool]['result'].get('login_pages', []):
                        urls.append(page)
        return list(set(urls))
    
    def _generate_manual_checklist(self):
        """Original implementation unchanged"""
        checklist = [
            "### Critical Areas for Manual Testing",
            "1. Authentication bypass on login endpoints",
            "2. Business logic flaws in payment flows",
            "3. IDOR in user-facing endpoints",
            "4. DOM-based XSS in complex JavaScript",
            "5. Race conditions in high-value transactions",
            "",
            "### Sensitive Domains to Investigate:",
            *[f"- {domain}" for domain in self.results.get('sensitive_domains', [])],
            "",
            "### OWASP Top 10 Coverage:",
            "✅ Injection, Broken Auth, Sensitive Data Exposure",
            "⚠️ XXE, Broken Access Control, Security Misconfig",
            "❌ Insecure Deserialization, Insufficient Logging"
        ]
        with open("outputs/manual_checklist.md", "w") as f:
            f.write("\n".join(checklist))
    
    def _monitor_resources(self):
        """Original implementation unchanged"""
        while self.is_running:
            cpu, mem, net_sent, net_recv = self.resource_monitor.get_usage()
            self.dashboard.update_resource_usage(cpu, mem, net_sent, net_recv)
            time.sleep(1)
    
    def _generate_tool_summary(self, tool_name, result):
        """Original implementation unchanged"""
        summaries = {
            "Sublist3r": lambda r: f"🌐 Found {len(r.get('subdomains', []))} subdomains",
            "Amass": lambda r: f"🕵️ Discovered {len(r.get('subdomains', []))} subdomains",
            "Nuclei": lambda r: f"⚠️ Found {len(r.get('vulnerabilities', []))} vulnerabilities",
            "CloudScanner": lambda r: f"☁️ Found {len(r.get('iam_findings', []))} IAM issues",
            "APISecurity": lambda r: f"🔒 Found {len(r.get('idor_issues', []))} auth issues",
            "DarkWebIntel": lambda r: f"🌑 Found {len(r.get('market_listings', []))} dark web listings",
            "SecretFinder": lambda r: f"🔑 Found {len(r.get('secrets', []))} potential secrets",
            "DNSEnumerator": lambda r: f"🔍 Found {sum(len(v) for v in r.values())} DNS records",
            "Wayback": lambda r: f"🕰️ Archived {len(r)} historical snapshots",
            "CryptoMonitor": lambda r: f"💰 Monitored {len(r)} cryptocurrency addresses",
            "MITREMapper": lambda r: f"🔰 Mapped {len(r)} MITRE ATT&CK techniques"
        }
        return summaries.get(tool_name, lambda r: "✅ Completed successfully")(result)
    
    def generate_report(self, filename):
        """Original implementation unchanged"""
        from core.report_generator import generate_html_report
        report_data = {
            'target': self.target,
            'mode': self.mode,
            'start_time': self.state['start_time'],
            'end_time': self.state.get('end_time', datetime.now().isoformat()),
            'results': self.results,
            'errors': self.errors,
            'stats': self._calculate_stats(),
            'correlation': self.results.get('correlation', {}),
            'visualization': f"visualizations/{self.target}_attack_surface.html"
        }
        return generate_html_report(report_data, filename)
    
    def _calculate_stats(self):
        """Original implementation unchanged"""
        stats = {
            'subdomains': 0,
            'assets': 0,
            'vulnerabilities': 0,
            'secrets': 0,
            'cloud_issues': 0,
            'api_issues': 0,
            'dns_records': 0,
            'crypto_addresses': 0,
            'mitre_techniques': 0
        }
        for phase, tools in self.results.items():
            for tool, data in tools.items():
                result = data['result']
                if 'subdomains' in result:
                    stats['subdomains'] += len(result['subdomains'])
                if 'vulnerabilities' in result:
                    stats['vulnerabilities'] += len(result['vulnerabilities'])
                if 'secrets' in result:
                    stats['secrets'] += len(result['secrets'])
                if 'iam_findings' in result:
                    stats['cloud_issues'] += len(result['iam_findings'])
                if 's3_findings' in result:
                    stats['cloud_issues'] += len(result['s3_findings'])
                if 'idor_issues' in result:
                    stats['api_issues'] += len(result['idor_issues'])
                if 'dns_records' in result:
                    for domain in result['dns_records']:
                        stats['dns_records'] += len(result['dns_records'][domain])
        if 'crypto_monitoring' in self.results:
            stats['crypto_addresses'] = len(self.results['crypto_monitoring'])
        if 'mitre_mapping' in self.results:
            stats['mitre_techniques'] = len(self.results['mitre_mapping'])
        return stats
    
    def get_current_state(self):
        """Original implementation unchanged"""
        return self.state
    
    def stop_scan(self):
        """Enhanced with thread-safe state saving"""
        with filelock.FileLock("state.lock", timeout=5):
            self.is_running = False
            save_state(self.state)
            self.dashboard.show_warning("🛑 Scan stopped. State securely saved.")

    # NEW HELPER METHODS =====================================================
    
    def extract_crypto_addresses(self):
        """Enhanced cryptocurrency detection"""
        addresses = set()
        content = json.dumps(self.results)
        for pattern in CRYPTO_PATTERNS.values():
            addresses.update(re.findall(pattern, content))
        return list(addresses)

    async def monitor_crypto_addresses(self, addresses):
        """Async blockchain monitoring with multi-chain support"""
        results = {}
        async with aiohttp.ClientSession() as session:
            tasks = []
            for address in addresses:
                # Determine chain type
                chain_type = None
                for crypto, pattern in CRYPTO_PATTERNS.items():
                    if re.match(pattern, address):
                        chain_type = crypto
                        break
                
                if not chain_type:
                    results[address] = {"error": "Unsupported cryptocurrency"}
                    continue
                    
                # Dispatch to appropriate handler
                if chain_type == "ETH":
                    tasks.append(self._check_etherscan(session, address))
                elif chain_type == "BTC":
                    tasks.append(self._check_blockchain(session, address))
                elif chain_type == "TRX":
                    tasks.append(self._check_tronscan(session, address))
                elif chain_type == "BSC":
                    tasks.append(self._check_bscscan(session, address))
                else:
                    results[address] = {"error": "Handler not implemented"}
            
            # Process all tasks concurrently
            completed = await asyncio.gather(*tasks, return_exceptions=True)
            for result in completed:
                if not isinstance(result, Exception):
                    results.update(result)
        
        # Optional Chainalysis integration
        if os.getenv("ENABLE_CHAINALYSIS"):
            try:
                from .chainalysis_integration import risk_assessment
                risk_scores = risk_assessment(list(results.keys()))
                for addr, risk in risk_scores.items():
                    if addr in results:
                        results[addr]["risk_score"] = risk
            except ImportError:
                pass
        
        return results

    async def _check_etherscan(self, session, address):
        """Ethereum blockchain checker"""
        try:
            async with session.get(
                "https://api.etherscan.io/api",
                params={
                    "module": "account",
                    "action": "balance",
                    "address": address,
                    "tag": "latest",
                    "apikey": os.getenv("ETHERSCAN_API_KEY", "freekey")
                },
                timeout=10
            ) as response:
                data = await response.json()
                return {
                    address: {
                        "type": "Ethereum",
                        "balance": int(data['result']) / 10**18,
                        "transactions": "N/A"  # Actual implementation would fetch txs
                    }
                }
        except Exception as e:
            return {address: {"error": str(e)}}

    # Similar handlers for BTC, TRX, BSC would go here...

    def map_to_mitre_attack(self):
        """Comprehensive MITRE ATT&CK mapping"""
        from .mitre_mapper import MitreMapper
        
        mapper = MitreMapper()
        for phase in self.results:
            for tool in self.results[phase]:
                if "vulnerabilities" in self.results[phase][tool]:
                    for vuln in self.results[phase][tool]["vulnerabilities"]:
                        mapper.add_finding(
                            vuln_type=vuln.get("type"),
                            severity=vuln.get("severity"),
                            context={
                                "target": self.target,
                                "tool": tool,
                                "phase": phase
                            }
                        )
        return mapper.get_techniques()

class FalsePositiveReducer:
    def __init__(self):
        self.model = self._select_model()
        self.features = ['response_length', 'status_code', 'word_count', 'entropy']
    
    def _select_model(self):
        """Choose appropriate ML model based on environment"""
        if os.getenv("ENABLE_DIFFPRIVACY"):
            return DPIsolationForest(epsilon=0.5, data_norm=1000)
        return IsolationForest(contamination=0.1, random_state=42)
    
    def train(self, training_data):
        X = np.array([[f[feat] for feat in self.features] for f in training_data])
        y = np.array([f.get('valid', 1) for f in training_data])
        self.model.fit(X, y)
    
    def predict(self, vulnerability):
        features = {
            'response_length': len(vulnerability.get('response', '')),
            'status_code': vulnerability.get('status', 0),
            'word_count': len(vulnerability.get('description', '').split()),
            'entropy': self.calculate_entropy(vulnerability.get('response', ''))
        }
        feature_vec = [features.get(f, 0) for f in self.features]
        return self.model.predict([feature_vec])[0] == 1
    
    def calculate_entropy(self, text):
        import math
        if not text:
            return 0
        entropy = 0
        for x in range(256):
            p_x = float(text.count(chr(x))) / len(text)
            if p_x > 0:
                entropy += - p_x * math.log(p_x, 2)
        return entropy


phase_workflow.py

def get_workflow(mode, custom_tools=None):
    """Enhanced workflow with conditional execution"""
    # ... existing code ...
    
    if mode == "deeper":
        workflow.append({
            "name": "Cloud Infrastructure Mapping",
            "description": "Discover cloud assets and configurations",
            "tools": [{"name": "cloud_scanner", "critical": True}],
            "conditions": lambda r: any(
                t in r.get('subdomains', {}) 
                for t in ['amass', 'sublist3r']
            )
        })
        workflow.append({
            "name": "Dark Web Presence Check",
            "description": "Monitor dark web for target mentions",
            "tools": [{"name": "darkweb_monitor"}],
            "conditions": lambda r: "sensitive_domains" in r
        })
    
    # Add correlation phase
    workflow.append({
        "name": "Threat Correlation",
        "description": "Analyze relationships between entities",
        "tools": [{"name": "correlation_engine"}]
    })
    
    return workflow


report_generator.py


import json
import os
import jinja2
from datetime import datetime

REPORT_TEMPLATE = "config/templates/report.html.j2"
OUTPUT_DIR = "outputs/reports"

def generate_html_report(report_data, filename):
    """Generate HTML report from template"""
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # Calculate durations
    start_time = datetime.fromisoformat(report_data['start_time'])
    end_time = datetime.fromisoformat(report_data.get('end_time', datetime.now().isoformat()))
    duration = end_time - start_time
    
    # Prepare data for template
    report_data['duration'] = str(duration)
    report_data['stats'] = report_data.get('stats', {})
    
    # Load template
    template_loader = jinja2.FileSystemLoader(searchpath="./")
    template_env = jinja2.Environment(loader=template_loader)
    
    try:
        template = template_env.get_template(REPORT_TEMPLATE)
    except Exception as e:
        print(f"Error loading template: {str(e)}")
        return None
    
    # Render report
    html = template.render(report_data)
    
    # Save to file
    report_path = os.path.join(OUTPUT_DIR, f"{filename}.html")
    with open(report_path, 'w') as f:
        f.write(html)
    
    return report_path

def generate_text_summary(report_data):
    """Generate text summary of results"""
    summary = [
        f"NightOwl Reconnaissance Report",
        f"Target: {report_data['target']}",
        f"Scan Mode: {report_data['mode']}",
        f"Start Time: {report_data['start_time']}",
        f"End Time: {report_data.get('end_time', 'Incomplete')}",
        f"Duration: {report_data.get('duration', 'N/A')}",
        "",
        "=== Summary ===",
        f"Subdomains Found: {report_data['stats'].get('subdomains', 0)}",
        f"Critical Assets: {report_data['stats'].get('critical_assets', 0)}",
        f"Vulnerabilities Found: {report_data['stats'].get('vulnerabilities', 0)}",
        f"Secrets Found: {report_data['stats'].get('secrets', 0)}",
        "",
        "=== Next Steps ===",
        "1. Review important findings in outputs/important/",
        "2. Validate vulnerabilities in outputs/vulnerabilities/",
        "3. Check manual_testing_checklist.md for critical areas"
    ]
    
    return "\n".join(summary)


resource_manager.py

import psutil
import asyncio
import time
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor

class AdaptiveExecutor:
    def __init__(self, max_threads=50, max_processes=8):
        self.cpu_threshold = 75
        self.mem_threshold = 85
        self.max_threads = max_threads
        self.max_processes = max_processes
        self.last_adjust = time.time()
        self.cooldown = 30  # seconds between adjustments
        self.task_queue = asyncio.Queue()
        self.worker_tasks = []
        
    async def start_workers(self):
        """Start worker tasks for adaptive execution"""
        for _ in range(self.max_threads):
            task = asyncio.create_task(self._worker())
            self.worker_tasks.append(task)
    
    async def stop_workers(self):
        """Stop all worker tasks"""
        for _ in range(self.max_threads):
            await self.task_queue.put(None)
        await asyncio.gather(*self.worker_tasks, return_exceptions=True)
    
    async def execute_tool(self, tool_func, *args):
        """Execute tool with adaptive resource management"""
        current_load = self.system_load()
        task_id = id(tool_func)
        
        # Put task in queue
        await self.task_queue.put((tool_func, args, task_id))
        
        # Wait for completion
        while True:
            await asyncio.sleep(0.1)
            if task_id in self.completed_tasks:
                result = self.completed_tasks.pop(task_id)
                if isinstance(result, Exception):
                    raise result
                return result
    
    async def _worker(self):
        """Worker task that processes tool executions"""
        self.completed_tasks = {}
        while True:
            item = await self.task_queue.get()
            if item is None:
                break
                
            tool_func, args, task_id = item
            current_load = self.system_load()
            
            try:
                if current_load['cpu'] > self.cpu_threshold:
                    # Use threading for I/O bound tasks
                    with ThreadPoolExecutor(max_workers=self.dynamic_thread_count()) as executor:
                        loop = asyncio.get_running_loop()
                        result = await loop.run_in_executor(executor, tool_func, *args)
                elif current_load['mem'] > self.mem_threshold:
                    # Use process pool for CPU-bound tasks
                    with ProcessPoolExecutor(max_workers=self.dynamic_process_count()) as executor:
                        loop = asyncio.get_running_loop()
                        result = await loop.run_in_executor(executor, tool_func, *args)
                else:
                    # Direct execution for normal conditions
                    result = tool_func(*args)
                    
                self.completed_tasks[task_id] = result
            except Exception as e:
                self.completed_tasks[task_id] = e
            finally:
                self.task_queue.task_done()
    
    def system_load(self):
        return {
            "cpu": psutil.cpu_percent(),
            "mem": psutil.virtual_memory().percent
        }
    
    def dynamic_thread_count(self):
        load = self.system_load()
        if time.time() - self.last_adjust > self.cooldown:
            if load['cpu'] > 90:
                self.max_threads = max(10, self.max_threads - 5)
            elif load['cpu'] < 50:
                self.max_threads = min(50, self.max_threads + 5)
            self.last_adjust = time.time()
        return self.max_threads
    
    def dynamic_process_count(self):
        load = self.system_load()
        if time.time() - self.last_adjust > self.cooldown:
            if load['mem'] > 90:
                self.max_processes = max(2, self.max_processes - 2)
            elif load['mem'] < 60:
                self.max_processes = min(8, self.max_processes + 2)
            self.last_adjust = time.time()
        return self.max_processes

class MemoryOptimizer:
    def __init__(self, max_size=1000, default_ttl=300):
        self.cache = {}
        self.cache_expiry = {}
        self.max_size = max_size
        self.default_ttl = default_ttl
        self.hits = 0
        self.misses = 0
    
    def cache_result(self, key, value, ttl=None):
        if len(self.cache) >= self.max_size:
            self._evict_oldest()
        ttl = ttl or self.default_ttl
        self.cache[key] = value
        self.cache_expiry[key] = time.time() + ttl
    
    def get_cached(self, key):
        if key in self.cache:
            if self.cache_expiry.get(key, 0) > time.time():
                self.hits += 1
                return self.cache[key]
            else:
                del self.cache[key]
                del self.cache_expiry[key]
        self.misses += 1
        return None
    
    def _evict_oldest(self):
        if not self.cache_expiry:
            return
            
        oldest_key = min(self.cache_expiry, key=self.cache_expiry.get)
        del self.cache[oldest_key]
        del self.cache_expiry[oldest_key]
    
    def clear_cache(self):
        self.cache.clear()
        self.cache_expiry.clear()
        self.hits = 0
        self.misses = 0
    
    def cache_stats(self):
        return {
            "size": len(self.cache),
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": self.hits / (self.hits + self.misses) if self.hits + self.misses > 0 else 0
        }


resource_monitor.py

import psutil
import asyncio
import time
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor

class AdaptiveExecutor:
    def __init__(self, max_threads=50, max_processes=8):
        self.cpu_threshold = 75
        self.mem_threshold = 85
        self.max_threads = max_threads
        self.max_processes = max_processes
        self.last_adjust = time.time()
        self.cooldown = 30  # seconds between adjustments
    
    async def execute_tool(self, tool_func, *args):
        current_load = self.system_load()
        
        if current_load['cpu'] > self.cpu_threshold:
            # Use threading for I/O bound tasks
            with ThreadPoolExecutor(max_workers=self.dynamic_thread_count()) as executor:
                loop = asyncio.get_running_loop()
                return await loop.run_in_executor(executor, tool_func, *args)
        elif current_load['mem'] > self.mem_threshold:
            # Use process pool for CPU-bound tasks
            with ProcessPoolExecutor(max_workers=self.dynamic_process_count()) as executor:
                loop = asyncio.get_running_loop()
                return await loop.run_in_executor(executor, tool_func, *args)
        else:
            # Direct execution for normal conditions
            return tool_func(*args)
    
    def system_load(self):
        return {
            "cpu": psutil.cpu_percent(),
            "mem": psutil.virtual_memory().percent
        }
    
    def dynamic_thread_count(self):
        load = self.system_load()
        if time.time() - self.last_adjust > self.cooldown:
            if load['cpu'] > 90:
                self.max_threads = max(10, self.max_threads - 5)
            elif load['cpu'] < 50:
                self.max_threads = min(50, self.max_threads + 5)
            self.last_adjust = time.time()
        return self.max_threads
    
    def dynamic_process_count(self):
        load = self.system_load()
        if time.time() - self.last_adjust > self.cooldown:
            if load['mem'] > 90:
                self.max_processes = max(2, self.max_processes - 2)
            elif load['mem'] < 60:
                self.max_processes = min(8, self.max_processes + 2)
            self.last_adjust = time.time()
        return self.max_processes

class MemoryOptimizer:
    def __init__(self):
        self.cache = {}
        self.cache_size = 1000
        self.cache_expiry = {}
    
    def cache_result(self, key, value, ttl=300):
        if len(self.cache) >= self.cache_size:
            self.expire_cache()
        self.cache[key] = value
        self.cache_expiry[key] = time.time() + ttl
    
    def get_cached(self, key):
        if key in self.cache and self.cache_expiry.get(key, 0) > time.time():
            return self.cache[key]
        return None
    
    def expire_cache(self):
        current_time = time.time()
        expired_keys = [k for k, exp in self.cache_expiry.items() if exp < current_time]
        for key in expired_keys[:100]:
            del self.cache[key]
            del self.cache_expiry[key]
    
    def clear_cache(self):
        self.cache.clear()
        self.cache_expiry.clear()


state_manager.py

import json
import os
import pickle
from datetime import datetime

STATE_FILE = ".nightowl_state"

def save_state(state_data, filename=STATE_FILE):
    """Save current scan state to file"""
    try:
        with open(filename, 'wb') as f:
            pickle.dump(state_data, f)
        return True
    except Exception as e:
        print(f"Error saving state: {str(e)}")
        return False

def load_state(filename=STATE_FILE):
    """Load scan state from file"""
    if not os.path.exists(filename):
        return None
        
    try:
        with open(filename, 'rb') as f:
            return pickle.load(f)
    except Exception as e:
        print(f"Error loading state: {str(e)}")
        return None

def clear_state(filename=STATE_FILE):
    """Clear saved state"""
    if os.path.exists(filename):
        os.remove(filename)
        return True
    return False


tool_runner.py

import subprocess
import json
import os
import importlib
from core.error_handler import ErrorHandler
from core.phase_workflow import get_tool_config

class ToolRunner:
    def __init__(self):
        self.output_dir = "outputs/scans"
        os.makedirs(self.output_dir, exist_ok=True)
    
    def run(self, tool_name, target, mode, progress_callback=None):
        """Execute a tool and return parsed results"""
        config = get_tool_config(tool_name)
        if not config:
            raise ValueError(f"No configuration found for tool: {tool_name}")
        
        # Handle Python-based tools
        if tool_name.startswith("python_"):
            return self._run_python_tool(tool_name, target, progress_callback)
        
        # Build command
        command = config["command"].format(
            target=target,
            output=os.path.join(self.output_dir, f"{tool_name}_{target}.json")
        )
        
        # Execute command
        try:
            if progress_callback:
                progress_callback(tool_name, 10, "Starting...")
            
            result = subprocess.run(
                command,
                shell=True,
                capture_output=True,
                text=True,
                check=True
            )
            
            if progress_callback:
                progress_callback(tool_name, 80, "Processing output...")
            
            # Parse results
            parser_name = config.get("parser", "")
            if parser_name:
                return self._parse_output(parser_name, result.stdout)
            return {"raw": result.stdout}
        
        except subprocess.CalledProcessError as e:
            ErrorHandler.log_error(
                tool_name, 
                f"Command failed: {e.stderr or e.stdout}", 
                target
            )
            raise RuntimeError(f"{tool_name} execution failed") from e
    
    def _run_python_tool(self, tool_name, target, progress_callback):
        """Execute a Python-based tool"""
        module_name = tool_name.replace("python_", "")
        try:
            module = importlib.import_module(f"tools.{module_name}")
            run_func = getattr(module, "run", None)
            if not run_func:
                raise AttributeError(f"No run function in {module_name}")
            
            return run_func(target, progress_callback)
        except Exception as e:
            ErrorHandler.log_error(tool_name, str(e), target)
            raise
    
    def _parse_output(self, parser_name, output):
        """Parse tool output using specified parser"""
        try:
            # Implement parsers for various tools
            if parser_name == "parse_json":
                return json.loads(output)
            elif parser_name == "parse_nuclei":
                return self._parse_nuclei_output(output)
            # Add more parsers as needed
            return {"raw": output}
        except Exception as e:
            ErrorHandler.log_error("ToolRunner", f"Parse error: {str(e)}", "")
            return {"raw": output, "parse_error": str(e)}
    
    def _parse_nuclei_output(self, output):
        """Parse Nuclei vulnerability scanner output"""
        try:
            vulns = []
            for line in output.splitlines():
                if line.strip():
                    vuln = json.loads(line.strip())
                    vulns.append({
                        "template": vuln.get("template-id", ""),
                        "severity": vuln.get("info", {}).get("severity", "unknown"),
                        "url": vuln.get("host", ""),
                        "description": vuln.get("info", {}).get("description", ""),
                        "type": vuln.get("type", "")
                    })
            return {"vulnerabilities": vulns}
        except Exception as e:
            return {"raw": output, "parse_error": str(e)}


vulnerability_scanner.py

import json
import os

class VulnerabilityScanner:
    OWASP_TOP_10 = {
        "A1": "Injection",
        "A2": "Broken Authentication",
        "A3": "Sensitive Data Exposure",
        "A4": "XXE",
        "A5": "Broken Access Control",
        "A6": "Security Misconfiguration",
        "A7": "XSS",
        "A8": "Insecure Deserialization",
        "A9": "Vulnerable Components",
        "A10": "Insufficient Logging"
    }
    
    def categorize_vulnerabilities(self, results):
        """Categorize vulnerabilities by type and OWASP Top 10"""
        vulnerabilities = {key: [] for key in self.OWASP_TOP_10.keys()}
        vulnerabilities["other"] = []
        
        # Extract all vulnerabilities from results
        all_vulns = []
        for phase, tools in results.items():
            for tool, data in tools.items():
                if "vulnerabilities" in data.get('result', {}):
                    all_vulns.extend(data['result']['vulnerabilities'])
        
        # Categorize each vulnerability
        for vuln in all_vulns:
            category = self._classify_vulnerability(vuln)
            if category:
                vulnerabilities[category].append(vuln)
            else:
                vulnerabilities["other"].append(vuln)
        
        # Save to vulnerability directory
        self._save_vulnerabilities(vulnerabilities)
        return vulnerabilities
    
    def _classify_vulnerability(self, vuln):
        """Classify vulnerability by OWASP Top 10"""
        vuln_type = vuln.get('type', '').lower()
        
        if "sql" in vuln_type or "command" in vuln_type or "nosql" in vuln_type:
            return "A1"
        elif "auth" in vuln_type or "session" in vuln_type:
            return "A2"
        elif "exposure" in vuln_type or "secret" in vuln_type:
            return "A3"
        elif "xxe" in vuln_type:
            return "A4"
        elif "access" in vuln_type or "idor" in vuln_type:
            return "A5"
        elif "config" in vuln_type or "misconfig" in vuln_type:
            return "A6"
        elif "xss" in vuln_type:
            return "A7"
        elif "deserialization" in vuln_type:
            return "A8"
        elif "component" in vuln_type or "dependency" in vuln_type:
            return "A9"
        elif "logging" in vuln_type or "monitoring" in vuln_type:
            return "A10"
        return None
    
    def _save_vulnerabilities(self, vulnerabilities):
        """Save vulnerabilities to categorized files"""
        for category, vulns in vulnerabilities.items():
            if vulns:
                with open(f"outputs/vulnerabilities/{category}.json", "w") as f:
                    json.dump(vulns, f, indent=2)


web3_integration.py

import time
import threading
from rich.console import Console
from rich.panel import Panel
from rich.progress import Progress, BarColumn, TextColumn
from rich.table import Table, Column
from rich.live import Live
from rich.text import Text
from rich.layout import Layout
from .theme import NIGHTOWL_THEME

class NightOwlDashboard:
    def __init__(self):
        self.console = Console(theme=NIGHTOWL_THEME)
        self.live = None
        self.layout = None
        self.progress_bars = {}
        self.current_phase = 0
        self.resource_thread = None
        self.running = False
        self.is_running = False
        
    def display_header(self):
        title = Text("🦉 NIGHT OWL RECONNAISSANCE SUITE", justify="center", style="banner")
        subtitle = Text("Comprehensive Attack Surface Discovery", justify="center", style="subtitle")
        self.console.print(Panel(title, subtitle=subtitle, style="header"))
        
    def start(self):
        self.is_running = True
        self._create_layout()
        self.live = Live(self.layout, refresh_per_second=10, console=self.console)
        self.live.__enter__()
        return self
        
    def stop(self):
        self.is_running = False
        if self.live:
            self.live.__exit__(None, None, None)
            
    def set_target_info(self, target, mode, target_type):
        self.target = target
        self.mode = mode
        self.target_type = target_type
        self._update_header()
        
    def set_phases(self, phases):
        self.phases = phases
        self.phase_status = ["pending"] * len(phases)
        self._update_phase_display()
        
    def set_current_phase(self, phase_index):
        self.current_phase = phase_index
        if phase_index < len(self.phase_status):
            self.phase_status[phase_index] = "running"
        self._update_phase_display()
        
    def complete_phase(self, phase_index):
        if phase_index < len(self.phase_status):
            self.phase_status[phase_index] = "completed"
        self._update_phase_display()
        
    def start_tool(self, tool_name, description):
        progress = Progress(
            TextColumn(f"[tool_name]{tool_name}[/]", width=20),
            BarColumn(bar_width=40),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TextColumn("•"),
            TextColumn("[description]{task.description}", overflow="ellipsis"),
            expand=True
        )
        task_id = progress.add_task(description, total=100)
        self.progress_bars[tool_name] = {
            'progress': progress,
            'task_id': task_id,
            'start_time': time.time(),
            'status': 'running'
        }
        self._update_tools_display()
        
    def update_tool_progress(self, tool_name, percentage, message=""):
        if tool_name in self.progress_bars:
            progress = self.progress_bars[tool_name]['progress']
            task_id = self.progress_bars[tool_name]['task_id']
            progress.update(task_id, completed=percentage, description=message)
            self._update_tools_display()
            
    def complete_tool(self, tool_name, summary, duration):
        if tool_name in self.progress_bars:
            progress = self.progress_bars[tool_name]['progress']
            task_id = self.progress_bars[tool_name]['task_id']
            progress.update(task_id, completed=100, description=summary)
            self.progress_bars[tool_name]['duration'] = duration
            self.progress_bars[tool_name]['status'] = 'completed'
            self._update_tools_display()
            
    def tool_error(self, tool_name, error_message):
        if tool_name in self.progress_bars:
            progress = self.progress_bars[tool_name]['progress']
            task_id = self.progress_bars[tool_name]['task_id']
            progress.update(task_id, description=f"[error]ERROR: {error_message}[/]")
            self.progress_bars[tool_name]['status'] = 'error'
            self._update_tools_display()
            
    def skip_tool(self, tool_name):
        self.progress_bars[tool_name] = {
            'status': 'skipped',
            'message': 'Skipped (already completed)'
        }
        self._update_tools_display()
            
    def update_resource_usage(self, cpu, memory, network_sent, network_recv):
        self.resource_usage = (
            f"CPU: {cpu}% | "
            f"RAM: {memory}% | "
            f"NET: ▲{network_sent/1024:.1f}KB/s ▼{network_recv/1024:.1f}KB/s"
        )
        self._update_resource_display()
        
    def show_success(self, message):
        self.console.print(Panel(message, title="[success]Success[/]", style="success"))
        
    def show_warning(self, message):
        self.console.print(Panel(message, title="[warning]Warning[/]", style="warning"))
        
    def show_error(self, message):
        self.console.print(Panel(message, title="[error]Error[/]", style="error"))
        
    def _create_layout(self):
        # Create main layout structure
        self.layout = Layout()
        self.layout.split(
            Layout(name="header", size=3),
            Layout(name="main", ratio=1)
        )
        self.layout["main"].split_row(
            Layout(name="work_area", ratio=3),
            Layout(name="phase_area", ratio=1)
        )
        
        # Initialize panels
        self.header_panel = Panel("", title="System Resources", style="header_panel")
        self.resource_panel = Panel("", style="resource_panel")
        self.work_panel = Panel("", title="[bold]Active Operations[/]", style="work_panel")
        self.phase_panel = Panel("", title="[bold]Workflow Progress[/]", style="phase_panel")
        
        # Update layout
        self.layout["header"].update(self.header_panel)
        self.layout["work_area"].update(self.work_panel)
        self.layout["phase_area"].update(self.phase_panel)
        
        # Initial content
        self._update_header()
        self._update_resource_display()
        self._update_tools_display()
        self._update_phase_display()
        
    def _update_header(self):
        title = Text(f"Target: [target]{self.target}[/] | Mode: [mode]{self.mode}[/] | Type: [type]{self.target_type}[/]", style="header_text")
        self.header_panel.renderable = title
        
    def _update_resource_display(self):
        if hasattr(self, 'resource_usage'):
            self.resource_panel.renderable = self.resource_usage
            
    def _update_tools_display(self):
        if not self.progress_bars:
            self.work_panel.renderable = Panel("No active operations", style="empty")
            return
            
        grid = Table.grid(expand=True)
        for tool, data in self.progress_bars.items():
            if data['status'] == 'skipped':
                grid.add_row(f"[skipped]⏩ {tool}: {data['message']}[/]")
            elif 'progress' in data:
                grid.add_row(data['progress'])
                if 'duration' in data:
                    grid.add_row(f"⏱️ [dim]Completed in {data['duration']:.2f}s[/]")
            elif data['status'] == 'error':
                grid.add_row(f"[error]❌ {tool}: Failed[/]")
        self.work_panel.renderable = grid
        
    def _update_phase_display(self):
        if not hasattr(self, 'phases'):
            return
            
        table = Table(
            Column(header="Phase", style="phase_header"),
            Column(header="Status", style="status_header"),
            expand=True
        )
        
        for idx, phase in enumerate(self.phases):
            status = self.phase_status[idx]
            if status == "pending":
                icon, style = "⏳", "phase_pending"
            elif status == "completed":
                icon, style = "✅", "phase_completed"
            else:  # running
                icon, style = "🦉", "phase_active"
                
            table.add_row(phase['name'], f"{icon} [bold]{status.capitalize()}[/]", style=style)
            
        self.phase_panel.renderable = table       